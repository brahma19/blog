---
layout: default
---

# Welcome to My GitHub Page

## About Me

Hello! I'm a **Data Engineer** with a strong focus on **Google Cloud Platform (GCP)** and **Hadoop services**. With a passion for big data and cloud computing.
A problem solver who loves to work with data.
Highly skilled and experienced data engineer with expertise in Big Data and modern data warehousing.
Extensive experience with data modeling, schema design, and database technologies.

**Professional Data Engineer**                                    **Professional Cloud Architect**
<div data-iframe-width="150" data-iframe-height="270" data-share-badge-id="68dda8cf-243e-4744-a88c-11f7f3503782" data-share-badge-host="https://www.credly.com"></div><script type="text/javascript" async src="//cdn.credly.com/assets/utilities/embed.js"></script>  <div data-iframe-width="150" data-iframe-height="270" data-share-badge-id="adf91648-9e89-4de6-bc95-aca60f51af78" data-share-badge-host="https://www.credly.com"></div><script type="text/javascript" async src="//cdn.credly.com/assets/utilities/embed.js"></script>

## Expertise

- **Google Cloud Platform (GCP)**
  - BigQuery
  - Cloud Storage
  - Dataflow
  - Dataproc
  - Pub/Sub
  - Cloud Composer

- **Hadoop Ecosystem**
  - HDFS
  - Hive
  - Pig
  - Spark
  - Oozie

- **Programming Languages**
  - Python
  - SQL
  - Bash

- **Skills**
  - Strong SQL skills with BigQuery optimization
  - Cloud financial operations (FinOps)
  - Datawarehousing
  - Dimensional modelling

## Projects

### 1. Real-Time Data Pipeline with GCP
Designed and implemented a real-time data pipeline using Google Cloud Pub/Sub, Dataflow, and BigQuery to process and analyze streaming data.

### 2. Hadoop Cluster Optimization
Optimized a large-scale Hadoop cluster, improving performance and reducing costs by fine-tuning resource allocation and implementing best practices.

### 3. Data Lake on GCP
Migrated on prem Hadoop workloads and Data to GCP and integrating with BigQuery and Dataproc for analytics and machine learning workloads.

### 4. BigQuery ELT for Wireless Customers
Developed an ELT process in BigQuery capable of processing **70 TB** of data per hour within a **10-minute** SLA. This pipeline aggregates session-level information and calculates hourly KPIs for wireless customers connected to cell sites.

### 5. Cost Optimization with Cloud FinOps
Implemented cost-saving measures across various GCP services, reduced project level cost by **45 %** by modernising spark/hadoop worklodas running on Dataproc Long Running clusters to Dataproc Serverless and Native BQ ELT.
Leveraging FinOps principles to optimize resource usage and reduce expenses.

## Blog

Stay tuned for blog posts on GCP, Hadoop, and data engineering best practices.

## Contact Me

- **Email:** [brahmanandsingh@ymail.com](mailto:brahmanandsingh@ymail.com)
- **LinkedIn:** [Brahmanand Singh](http://www.linkedin.com/in/brahmanandsingh)

Feel free to reach out to me for collaboration or if you have any questions about data engineering on GCP and Hadoop!

---

*This page was generated using [GitHub Pages](https://pages.github.com/).*
